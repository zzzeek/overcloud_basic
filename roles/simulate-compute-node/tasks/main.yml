# this runs on hypervisor, using hosts file that is infrared-specific

- name: config
  include_vars: "{{ playbook_dir }}/../config.yml"


# e.g.
# ip route add 10.0.10.0/24 via 192.168.22.33
- name: add route on hypervisor to vlan10 / external network via undercloud
  shell: |
    if [[ $( ip route show {{ external_network_cidr }} ) ]]; then
      ip route del {{ external_network_cidr }}
    fi
    ip route add {{ external_network_cidr }} via 192.168.22.1

- name: add route on hypervisor to vlan20 / internal network via undercloud
  shell: |
    if [[ $( ip route show {{ internalapi_network_cidr }} ) ]]; then
      ip route del {{ internalapi_network_cidr }}
    fi
    ip route add {{ internalapi_network_cidr }} via 192.168.22.1



- name: ensure docker installed
  package:
      name:
        - docker
        - python-docker

- name: enable and start docker
  systemd:
    name: docker
    enabled: yes
    daemon_reload: yes
    state: started

- name: create docker network bridge
  docker_network:
    driver: bridge
    # leave masquerade on - control plane will see packets
    # routed from external libvirt network 192.168.22.0 rather
    # than docker network 172.18.0.0 and this seems to help w/
    # packets on the 10.0.20.0 network 
    #driver_options:
    #  com.docker.network.bridge.enable_ip_masquerade: false
    state: present
    name: compute_nodes
    driver_options:
      com.docker.network.bridge.name: compute_net
    ipam_options:
      subnet: '172.18.0.0/16'
      gateway: '172.18.0.1'

- name: create docker build location
  file:
    path: "/usr/local/dockerbuilds/compute_node"
    state: directory

- name: copy Dockerfile etc to docker build location
  copy:
    src: "{{ item }}"
    dest: "/usr/local/dockerbuilds/compute_node/{{ item }}"
  with_items:
    - "Dockerfile"
    - "entrypoint.sh"
    - "write_config.py"
    - "neutron.conf.fragment"
    - "nova.conf.fragment"

- name:  write hosts file
  copy: 
    content: "{{ hostvars['compute-0']['compute_hosts'] }}" 
    dest: /usr/local/dockerbuilds/compute_node/hosts

- name: write config tar.gz
  copy:
    src: /tmp/compute_config.tar.gz
    dest: /usr/local/dockerbuilds/compute_node/compute_config.tar.gz

- name: untar config
  shell: |
    cd /usr/local/dockerbuilds/compute_node
    tar -xf compute_config.tar.gz


- name: create docker image
  docker_image:
    name: compute_node
    path: "/usr/local/dockerbuilds/compute_node/"
    force: yes
    buildargs:
      delorean_url: "{{ delorean_url }}"
      delorean_deps_url: "{{ delorean_deps_url }}"

# note the entrypoint gets the ip number using `hostname -i`, so
# the container has to have just the one IP address on 172.18.0.0/16
# so purge_networks ensures this.   otherwise need to use ip route with
# grepping/ sed to get the correct ip number
- name: run a series of compute nodes
  docker_container:
    image: compute_node
    name: "{{ container_name }}"
    restart_policy: always
    recreate: yes
    purge_networks: yes
    capabilities:
      - ALL
      - NET_ADMIN
      - SYS_ADMIN
    privileged: yes
    hostname: "{{ hostname }}"
    networks:
      - name: compute_nodes    
    volumes:
      - "/usr/local/dockerbuilds/compute_node/hosts:/etc/hosts_compute:ro"
      - "/usr/local/dockerbuilds/compute_node/neutron/etc/neutron:/etc/neutron/original:ro"
      - "/usr/local/dockerbuilds/compute_node/nova_libvirt/etc/nova:/etc/nova/original/:ro"
  vars:
    container_name: "fake_node_{{ item }}"
    hostname: "fake-node-{{ item }}"
  with_sequence: start=1 end=5

  
# the command using docker looks like:
# docker run -d  --network compute_nodes -v "/usr/local/dockerbuilds/compute_node/hosts:/etc/hosts_compute:ro" -v "/usr/local/dockerbuilds/compute_node/neutron/etc/neutron:/etc/neutron/original:ro" -v "/usr/local/dockerbuilds/compute_node/nova_libvirt/etc/nova:/etc/nova/original/:ro" compute_node
#


