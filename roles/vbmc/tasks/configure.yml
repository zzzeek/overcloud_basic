
- block:

    - name: create selinux role for virtualbmc pid file
      script: create_selinux.sh
      become: true

    - name: Add VirtualBMC as systemd service
      become: true
      copy:
        mode: 0664
        dest: "/usr/lib/systemd/system/virtualbmc.service"
        content: |
          [Unit]
          Description=VirtualBMC service
          After=network.target
          [Service]
          User=stack
          ExecStart=/usr/bin/vbmcd
          ExecStop=/bin/bash -c 'for bmc in $(ls {{ ansible_user_dir }}/.vbmc/); do vbmc stop $bmc; done;'
          PIDFile={{ ansible_user_dir }}/.vbmc/master.pid
          Type=forking
          [Install]
          WantedBy=multi-user.target

    # TODO: rhel8 is being weird on this, a bunch of manual stopping/starting seemed to fix
    # it
    - name: Start the Virtual BMCs
      become: true
      systemd:
        name: virtualbmc
        state: started
        daemon_reload: true
        enabled: true

# crazy what's going on with vbmc command.  OK.   So it has an automatic
# daemonize feature, such that any time you run a vbmc command, if the "daemon"
# isn't running, it starts it up and puts the pid inside of ~/.vmbc/master.pid.
# However.  **the process in the ps list shows YOUR ACTUAL COMMAND running, and **NOT**
# anything that looks like a daemon**, which means it looks like your command hung, or something.
# IT DID NOT!  all is well.   This would be a great thing to fix in vbmc, but it's status is
# non-production grade, so here we are.
- name: stop existing Virtual BMCs
  shell: |
      for running in $( vbmc --no-daemon list -c "Domain name" -c "Status" -f value | grep running | awk "{print \$1}" ) ; do
        vbmc --no-daemon stop $running
      done

- name: delete all Virtual BMCs
  shell: |
      for node in $(vbmc --no-daemon list -c "Domain name" -f value) ; do
        vbmc --no-daemon delete $node
      done

- name: Get current count of used vbmc ports
  shell: vbmc --no-daemon list | grep 62 | wc -l
  register: vbmc_used_ports

- name: Define vbmc_port for each VM
  set_fact:
      vbmc_ports: "{{ vbmc_ports|default({})|combine({ item.1: vbmc_start_port + vbmc_used_ports.stdout|int + item.0 }) }}"
  with_indexed_items: "{{ vbmc_nodes }}"

- include_tasks: iptables.yml

- name: Set management bmc address
  set_fact:
      vbmc_management_address: "{{ ansible_default_ipv4.address }}"

- name: Add nodes to vbmc
  command: "vbmc --no-daemon add {{ item.key }} --port {{ item.value }} --username {{ vbmc_username }} --password {{ vbmc_password }} --address ::ffff:{{ vbmc_management_address }} \
            --libvirt-uri \"qemu+ssh://{{ hostvars['hypervisor'].ansible_user_id }}@{{ management_ipv4_address }}/system?no_verify=1&no_tty=1\""
  with_dict: "{{ vbmc_ports }}"

- name: Start nodes using vbmc
  command: "vbmc --no-daemon start {{ item.key }}"
  with_dict: "{{ vbmc_ports }}"

- name: Setting nodes power status using ipmitool
  command: "ipmitool -I lanplus -U {{ vbmc_username }} -P {{ vbmc_password }} -H {{ vbmc_management_address }} -p {{ item.value }} chassis power on"
  with_dict: "{{ vbmc_ports }}"
  register: command_result
  until: command_result.rc == 0
  retries: 5
  delay: 5

- name: Get vbmc VMs power status using ipmitool
  command: "ipmitool -I lanplus -U {{ vbmc_username }} -P {{ vbmc_password }} -H {{ vbmc_management_address }} -p {{ item.value }} power status"
  register: ipmitool_results
  with_dict: "{{ vbmc_ports }}"
  until: ipmitool_results.rc == 0
  retries: 5
  delay: 5

- name: Validate that all VMs are powered on
  fail:
      msg: "Node is not powered on! (ipmitool stdout: '{{ item.stdout }}')"
  when: "item.stdout != 'Chassis Power is on'"
  with_items: "{{ ipmitool_results.results }}"
