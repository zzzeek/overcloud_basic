- name: Retrieve public key from private key to localhost
  command: "ssh-keygen -y -f ~/.ssh/id_rsa"
  register: uc_pubkey_result

- name: insert the public key to the known hosts in hypervisor
  authorized_key:
      user: "{{ hostvars['hypervisor'].ansible_user|default(hostvars['hypervisor'].ansible_ssh_user) }}"
      key: "{{ uc_pubkey_result.stdout }}"
  delegate_to: hypervisor

- name: insert the public key to the known hosts in hypervisor for stack user
  authorized_key:
      user: stack
      key: "{{ uc_pubkey_result.stdout }}"
  delegate_to: hypervisor
  when: release_numeric_version < 11

- name: Get overcloud nodes
  set_fact:
    oc_nodes: "{{ overcloud_nodes }}"

# TODO: vbmc stuff is redundant here
- name: get vbmc ports
  shell: |
    vbmc list -c "Domain name" -c "Port" -f value
  register: vbmc_names_cmd

- name: Define vbmc_port for each VM
  set_fact:
      vbmc_ports: "{{ vbmc_ports|default({})|combine({ item.split()[0]: item.split()[1] }) }}"
  with_items: "{{ vbmc_names_cmd.stdout_lines }}"

- name: Set management bmc address
  set_fact:
      vbmc_management_address: "{{ ansible_default_ipv4.address }}"

- name: grab undercloud private key
  command: "cat ~/.ssh/id_rsa"
  register: uc_pkey_result

- name: discover provsion network name
  include_tasks: discover_provision_net.yml
  vars:
      uc_provision_net: "ctlplane"
  when: provison_virsh_network_name is not defined

- fail:
      msg: "The provision network cannot be discovered. Please rerun infrared command with -e provison_virsh_network_name=<net_name>"
  when: provison_virsh_network_name is not defined

- name: get information about vm's from hypervisor
  vars:
      vm_nodes: "{{ oc_nodes }}"
  delegate_to: hypervisor
  shell: |
      NODE_XML=`virsh dumpxml {{ item }}`
      disks_list="["
      for k in $(virsh domblklist {{ item }} | tail -n +3 | awk '{print $1}'); do
          disks_list="${disks_list}\"${dsk}\","
      done
      disks_list="${disks_list}]"
      disks_list="$(echo ${disks_list} | sed 's/,]/]/g')"

      echo "{
          'name': '{{ item }}',
          'arch': '`echo "$NODE_XML" | grep arch | cut -d\' -f2`',
          'memory_kibs': '`echo "$NODE_XML" | grep currentMemory | cut -d\< -f2 | cut -d\> -f2`',
          'mac': '`echo "$NODE_XML" | grep {{ provison_virsh_network_name }} -B 1 | grep mac | cut -d\' -f2`',
          'cpu': '`echo "$NODE_XML" | grep vcpu | cut -d\< -f2 | cut -d\> -f2`',
          'disk_bytes': '`virsh domblkinfo {{ item }} vda | grep -e Capacity | cut -d\: -f2 | xargs`',
          'disks': '${disks_list}',
      }"
  with_items: "{{ vm_nodes|sort }}"
  register: nodes_info
  tags:
      - skip_ansible_lint

- name: prepare instackenv.json file
  vars:
      instack_output: "{{ working_dir }}/instackenv.json"
      # json file shouldn't hold newlines
      undercloud_private_key: "{{ uc_pkey_result.stdout_lines | join('\\n')}}"
  template:
      src: instackenv.json.j2
      dest: "{{ instack_output }}"

# Using delegete here because of
# https://github.com/ansible/ansible/issues/16972
# This issue causes an exception when that playbook is included with the false condition
- name: power off overcloud nodes
  vars:
      vm_nodes: "{{ oc_nodes }}"
  virt:
      name: "{{ item }}"
      state: destroyed
  with_items: "{{ vm_nodes }}"
  delegate_to: hypervisor
